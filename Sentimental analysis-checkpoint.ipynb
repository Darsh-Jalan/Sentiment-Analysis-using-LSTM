{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11b0d0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\darsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd    # to load dataset\n",
    "import numpy as np     # for mathematic equation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords # to get collection of stopwords\n",
    "nltk.download('stopwords')\n",
    "from sklearn.model_selection import train_test_split       # for splitting dataset\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  # to encode text to int\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences   # to do padding or truncating\n",
    "from tensorflow.keras.models import Sequential     # the model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense # layers of the architecture\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint   # save model\n",
    "from tensorflow.keras.models import load_model   # load saved model\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e61ee6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  review sentiment\n",
      "0      One of the other reviewers has mentioned that ...  positive\n",
      "1      A wonderful little production. <br /><br />The...  positive\n",
      "2      I thought this was a wonderful way to spend ti...  positive\n",
      "3      Basically there's a family where a little boy ...  negative\n",
      "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "...                                                  ...       ...\n",
      "49995  I thought this movie did a down right good job...  positive\n",
      "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
      "49997  I am a Catholic taught in parochial elementary...  negative\n",
      "49998  I'm going to have to disagree with the previou...  negative\n",
      "49999  No one expects the Star Trek movies to be high...  negative\n",
      "\n",
      "[50000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# importing the dataset\n",
    "data = pd.read_csv(r\"C:\\Users\\darsh\\OneDrive\\Desktop\\Uni\\Projects\\AI\\Sentimental-analysis-using-LSTM-master\\IMDB-Dataset.csv\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e834030",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2652c6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews\n",
      "0        [one, reviewers, mentioned, watching, oz, epis...\n",
      "1        [a, wonderful, little, production, the, filmin...\n",
      "2        [i, thought, wonderful, way, spend, time, hot,...\n",
      "3        [basically, family, little, boy, jake, thinks,...\n",
      "4        [petter, mattei, love, time, money, visually, ...\n",
      "                               ...                        \n",
      "49995    [i, thought, movie, right, good, job, it, crea...\n",
      "49996    [bad, plot, bad, dialogue, bad, acting, idioti...\n",
      "49997    [i, catholic, taught, parochial, elementary, s...\n",
      "49998    [i, going, disagree, previous, comment, side, ...\n",
      "49999    [no, one, expects, star, trek, movies, high, a...\n",
      "Name: review, Length: 50000, dtype: object \n",
      "\n",
      "Sentiment\n",
      "0        1\n",
      "1        1\n",
      "2        1\n",
      "3        0\n",
      "4        1\n",
      "        ..\n",
      "49995    1\n",
      "49996    0\n",
      "49997    0\n",
      "49998    0\n",
      "49999    0\n",
      "Name: sentiment, Length: 50000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def load_dataset():\n",
    "    df = pd.read_csv(r\"C:\\Users\\darsh\\OneDrive\\Desktop\\Uni\\Projects\\AI\\Sentimental-analysis-using-LSTM-master\\IMDB-Dataset.csv\")\n",
    "    x_data = df['review']       # Reviews/Input\n",
    "    y_data = df['sentiment']    # Sentiment/Output\n",
    "\n",
    "    # PRE-PROCESS REVIEW\n",
    "    x_data = x_data.replace({'<.*?>': ''}, regex = True)          # remove html tag\n",
    "    x_data = x_data.replace({'[^A-Za-z]': ' '}, regex = True)     # remove non alphabet\n",
    "    x_data = x_data.apply(lambda review: [w for w in review.split() if w not in english_stops])  # remove stop words\n",
    "    x_data = x_data.apply(lambda review: [w.lower() for w in review])   # lower case\n",
    "    \n",
    "    # ENCODE SENTIMENT -> 0 & 1\n",
    "    y_data = y_data.replace('positive', 1)\n",
    "    y_data = y_data.replace('negative', 0)\n",
    "\n",
    "    return x_data, y_data\n",
    "\n",
    "x_data, y_data = load_dataset()\n",
    "\n",
    "print('Reviews')\n",
    "print(x_data, '\\n')\n",
    "print('Sentiment')\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90b767be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set\n",
      "10606    [tim, krabbe, praised, author, het, gouden, ei...\n",
      "39200    [i, action, movie, fan, today, i, never, seen,...\n",
      "4578     [i, loved, thing, the, wonderful, thing, pink,...\n",
      "43343    [harem, suare, best, film, i, saw, year, bravo...\n",
      "44342    [grabbed, attention, netflix, instant, play, h...\n",
      "                               ...                        \n",
      "23482    [sarah, plain, tall, winters, end, best, movie...\n",
      "18017    [i, may, biased, i, author, novel, the, hungry...\n",
      "7755     [beat, strong, deaf, mutants, like, rex, voorh...\n",
      "21570    [dracula, epitome, painfully, cheesy, cinema, ...\n",
      "22407    [recap, doctor, markov, developed, new, theory...\n",
      "Name: review, Length: 40000, dtype: object \n",
      "\n",
      "9968     [i, saw, film, premier, sundance, since, ameri...\n",
      "45430    [shocking, in, i, saw, jury, gagarin, alive, h...\n",
      "8233     [this, movie, starts, showing, map, explaining...\n",
      "41914    [the, book, gets, stars, probably, contains, s...\n",
      "34620    [death, camp, opera, right, here, right, now, ...\n",
      "                               ...                        \n",
      "23345    [what, truly, moronic, movie, i, say, writer, ...\n",
      "9305     [brought, philip, larkin, life, way, worthy, g...\n",
      "48962    [the, premise, movie, much, going, however, de...\n",
      "31757    [the, original, grudge, original, american, re...\n",
      "45583    [i, seen, much, german, comedy, film, anything...\n",
      "Name: review, Length: 10000, dtype: object \n",
      "\n",
      "Test Set\n",
      "10606    1\n",
      "39200    1\n",
      "4578     1\n",
      "43343    1\n",
      "44342    1\n",
      "        ..\n",
      "23482    1\n",
      "18017    1\n",
      "7755     1\n",
      "21570    0\n",
      "22407    0\n",
      "Name: sentiment, Length: 40000, dtype: int64 \n",
      "\n",
      "9968     0\n",
      "45430    0\n",
      "8233     0\n",
      "41914    0\n",
      "34620    1\n",
      "        ..\n",
      "23345    0\n",
      "9305     1\n",
      "48962    0\n",
      "31757    0\n",
      "45583    1\n",
      "Name: sentiment, Length: 10000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.2)\n",
    "\n",
    "print('Train Set')\n",
    "print(x_train, '\\n')\n",
    "print(x_test, '\\n')\n",
    "print('Test Set')\n",
    "print(y_train, '\\n')\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bdd97b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length():\n",
    "    review_length = []\n",
    "    for review in x_train:\n",
    "        review_length.append(len(review))\n",
    "\n",
    "    return int(np.ceil(np.mean(review_length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c35f9d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded X Train\n",
      " [[ 1587 19462  5837 ...   594  1262 15804]\n",
      " [    1   115     3 ...     0     0     0]\n",
      " [    1   341    66 ...     0     0     0]\n",
      " ...\n",
      " [ 1351   469  5199 ...   140   209    44]\n",
      " [ 2397  9145  2179 ...     0     0     0]\n",
      " [11657   734 19402 ...   215   118 13888]] \n",
      "\n",
      "Encoded X Test\n",
      " [[   1  120    4 ... 9510 2967  121]\n",
      " [1451   50    1 ...  389   25  123]\n",
      " [   8    3  438 ...    0    0    0]\n",
      " ...\n",
      " [   2  754    3 ...    0    0    0]\n",
      " [   2  125 4135 ...  407  277  649]\n",
      " [   1   38   17 ...    0    0    0]] \n",
      "\n",
      "Maximum review length:  130\n"
     ]
    }
   ],
   "source": [
    "# ENCODE REVIEW\n",
    "token = Tokenizer(lower=False)    # no need lower, because already lowered the data in load_data()\n",
    "token.fit_on_texts(x_train)\n",
    "x_train = token.texts_to_sequences(x_train)\n",
    "x_test = token.texts_to_sequences(x_test)\n",
    "\n",
    "max_length = get_max_length()\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')\n",
    "x_test = pad_sequences(x_test, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "total_words = len(token.word_index) + 1   # add 1 because of 0 padding\n",
    "\n",
    "print('Encoded X Train\\n', x_train, '\\n')\n",
    "print('Encoded X Test\\n', x_test, '\\n')\n",
    "print('Maximum review length: ', max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e5d9559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 130, 32)           2957888   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                24832     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,982,785\n",
      "Trainable params: 2,982,785\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# ARCHITECTURE\n",
    "EMBED_DIM = 32\n",
    "LSTM_OUT = 64\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, EMBED_DIM, input_length = max_length))\n",
    "model.add(LSTM(LSTM_OUT))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "906aeb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\n",
    "    'models/LSTM.h5',\n",
    "    monitor='accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8bf7b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.4669 - accuracy: 0.7591\n",
      "Epoch 1: accuracy improved from -inf to 0.75912, saving model to models\\LSTM.h5\n",
      "313/313 [==============================] - 25s 74ms/step - loss: 0.4669 - accuracy: 0.7591\n",
      "Epoch 2/6\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.2115 - accuracy: 0.9234\n",
      "Epoch 2: accuracy improved from 0.75912 to 0.92343, saving model to models\\LSTM.h5\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.2115 - accuracy: 0.9234\n",
      "Epoch 3/6\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.1331 - accuracy: 0.9583\n",
      "Epoch 3: accuracy improved from 0.92343 to 0.95830, saving model to models\\LSTM.h5\n",
      "313/313 [==============================] - 25s 81ms/step - loss: 0.1331 - accuracy: 0.9583\n",
      "Epoch 4/6\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.0860 - accuracy: 0.9750\n",
      "Epoch 4: accuracy improved from 0.95830 to 0.97505, saving model to models\\LSTM.h5\n",
      "313/313 [==============================] - 25s 80ms/step - loss: 0.0860 - accuracy: 0.9750\n",
      "Epoch 5/6\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.0609 - accuracy: 0.9841\n",
      "Epoch 5: accuracy improved from 0.97505 to 0.98408, saving model to models\\LSTM.h5\n",
      "313/313 [==============================] - 25s 80ms/step - loss: 0.0609 - accuracy: 0.9841\n",
      "Epoch 6/6\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.0456 - accuracy: 0.9887\n",
      "Epoch 6: accuracy improved from 0.98408 to 0.98867, saving model to models\\LSTM.h5\n",
      "313/313 [==============================] - 25s 81ms/step - loss: 0.0456 - accuracy: 0.9887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2498302ee80>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size = 128, epochs = 6, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f2f45b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.0422 - accuracy: 0.9929\n"
     ]
    }
   ],
   "source": [
    "#evaluate our model\n",
    "result = model.evaluate(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dcfe26e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model('models/LSTM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71306a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie Review: One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\n"
     ]
    }
   ],
   "source": [
    "review = str(input('Movie Review: '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4bc2688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned:  One of the other reviewers has mentioned that after watching just  Oz episode youll be hooked They are right as this is exactly what happened with mebr br The first thing that struck me about Oz was its brutality and unflinching scenes of violence which set in right from the word GO Trust me this is not a show for the faint hearted or timid This show pulls no punches with regards to drugs sex or violence Its is hardcore in the classic use of the wordbr br It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary It focuses mainly on Emerald City an experimental section of the prison where all the cells have glass fronts and face inwards so privacy is not high on the agenda Em City is home to manyAryans Muslims gangstas Latinos Christians Italians Irish and moreso scuffles death stares dodgy dealings and shady agreements are never far awaybr br I would say the main appeal of the show is due to the fact that it goes where other shows wouldnt dare Forget pretty pictures painted for mainstream audiences forget charm forget romanceOZ doesnt mess around The first episode I ever saw struck me as so nasty it was surreal I couldnt say I was ready for it but as I watched more I developed a taste for Oz and got accustomed to the high levels of graphic violence Not just violence but injustice crooked guards wholl be sold out for a nickel inmates wholl kill on order and get away with it well mannered middle class inmates being turned into prison bitches due to their lack of street skills or prison experience Watching Oz you may become comfortable with what is uncomfortable viewingthats if you can get in touch with your darker side\n",
      "Filtered:  ['one reviewers mentioned watching  oz episode youll hooked they right exactly happened mebr br the first thing struck oz brutality unflinching scenes violence set right word go trust show faint hearted timid this show pulls punches regards drugs sex violence its hardcore classic use wordbr br it called oz nickname given oswald maximum security state penitentary it focuses mainly emerald city experimental section prison cells glass fronts face inwards privacy high agenda em city home manyaryans muslims gangstas latinos christians italians irish moreso scuffles death stares dodgy dealings shady agreements never far awaybr br i would say main appeal show due fact goes shows wouldnt dare forget pretty pictures painted mainstream audiences forget charm forget romanceoz doesnt mess around the first episode i ever saw struck nasty surreal i couldnt say i ready i watched i developed taste oz got accustomed high levels graphic violence not violence injustice crooked guards wholl sold nickel inmates wholl kill order get away well mannered middle class inmates turned prison bitches due lack street skills prison experience watching oz may become comfortable uncomfortable viewingthats get touch darker side']\n"
     ]
    }
   ],
   "source": [
    "# Pre-process input\n",
    "regex = re.compile(r'[^a-zA-Z\\s]')\n",
    "review = regex.sub('', review)\n",
    "print('Cleaned: ', review)\n",
    "\n",
    "words = review.split(' ')\n",
    "filtered = [w for w in words if w not in english_stops]\n",
    "filtered = ' '.join(filtered)\n",
    "filtered = [filtered.lower()]\n",
    "\n",
    "print('Filtered: ', filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a79aa8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    5  1918  1003    65  2842   295  3155   110   111   541   519 36865\n",
      "      2    23    66  3125  2842  5148 14821    60   476   172   111   558\n",
      "     62  1672    46  7648  2231 10714     8    46  2410  5611  5134  1431\n",
      "    278   476   744  3771   264   261 36865     7   337  2842 11463   259\n",
      "  13730  6915  2484   954 52679     7  2712  1278 26245   424  4976  2557\n",
      "   1116  6788  2872 12599   296 71373 16801   204  4936  2945   424   249\n",
      "   8577 44404 13041  4889  7377  2322 22080 43537   229  8966  6975 12201\n",
      "   8305 31266    43   132 36865     1    12    58   188  1231    46   590\n",
      "    102   174   183 24964  2976   737    90  1238  3968  2424  1157   737\n",
      "   1357   737 15769   867    93     2    23   295     1    51   120  3125\n",
      "   1538  2184     1 20786    58     1  1472     1   194     1]]\n"
     ]
    }
   ],
   "source": [
    "tokenize_words = token.texts_to_sequences(filtered)\n",
    "tokenize_words = pad_sequences(tokenize_words, maxlen=max_length, padding='post', truncating='post')\n",
    "print(tokenize_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ba46ae9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 354ms/step\n",
      "[[0.97369325]]\n"
     ]
    }
   ],
   "source": [
    "result = loaded_model.predict(tokenize_words)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e9ab13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "if result >= 0.7:\n",
    "    print('positive')\n",
    "else:\n",
    "    print('negative')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
